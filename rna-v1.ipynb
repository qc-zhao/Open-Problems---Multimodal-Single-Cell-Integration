{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --quiet tables","metadata":{"execution":{"iopub.status.busy":"2022-09-04T02:36:37.191057Z","iopub.execute_input":"2022-09-04T02:36:37.191539Z","iopub.status.idle":"2022-09-04T02:36:52.413692Z","shell.execute_reply.started":"2022-09-04T02:36:37.191446Z","shell.execute_reply":"2022-09-04T02:36:52.412009Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-09-04T02:36:52.416702Z","iopub.execute_input":"2022-09-04T02:36:52.417106Z","iopub.status.idle":"2022-09-04T02:36:53.101804Z","shell.execute_reply.started":"2022-09-04T02:36:52.417067Z","shell.execute_reply":"2022-09-04T02:36:53.100650Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"os.listdir(\"/kaggle/input/open-problems-multimodal/\")","metadata":{"execution":{"iopub.status.busy":"2022-09-04T02:36:53.103199Z","iopub.execute_input":"2022-09-04T02:36:53.103573Z","iopub.status.idle":"2022-09-04T02:36:53.114405Z","shell.execute_reply.started":"2022-09-04T02:36:53.103539Z","shell.execute_reply":"2022-09-04T02:36:53.113245Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":" # 1.   **Explorataory Data Analysis**","metadata":{}},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/open-problems-multimodal/\"\nFP_CELL_METADATA = os.path.join(DATA_DIR,\"metadata.csv\")\n\nFP_CITE_TRAIN_INPUTS = os.path.join(DATA_DIR,\"train_cite_inputs.h5\")\nFP_CITE_TRAIN_TARGETS = os.path.join(DATA_DIR,\"train_cite_targets.h5\")\nFP_CITE_TEST_INPUTS = os.path.join(DATA_DIR,\"test_cite_inputs.h5\")\n\nFP_MULTIOME_TRAIN_INPUTS = os.path.join(DATA_DIR,\"train_multi_inputs.h5\")\nFP_MULTIOME_TRAIN_TARGETS = os.path.join(DATA_DIR,\"train_multi_targets.h5\")\nFP_MULTIOME_TEST_INPUTS = os.path.join(DATA_DIR,\"test_multi_inputs.h5\")\n\nFP_SUBMISSION = os.path.join(DATA_DIR,\"sample_submission.csv\")\nFP_EVALUATION_IDS = os.path.join(DATA_DIR,\"evaluation_ids.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-09-04T02:36:53.118134Z","iopub.execute_input":"2022-09-04T02:36:53.118655Z","iopub.status.idle":"2022-09-04T02:36:53.126597Z","shell.execute_reply.started":"2022-09-04T02:36:53.118609Z","shell.execute_reply":"2022-09-04T02:36:53.125511Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_meta = pd.read_csv(FP_CELL_METADATA).set_index(\"cell_id\")\ndisplay(df_meta.head())","metadata":{"execution":{"iopub.status.busy":"2022-09-04T02:36:53.128220Z","iopub.execute_input":"2022-09-04T02:36:53.128571Z","iopub.status.idle":"2022-09-04T02:36:53.547207Z","shell.execute_reply.started":"2022-09-04T02:36:53.128541Z","shell.execute_reply":"2022-09-04T02:36:53.546013Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(f\"table size: {len(df_meta)}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-04T02:36:53.549059Z","iopub.execute_input":"2022-09-04T02:36:53.549476Z","iopub.status.idle":"2022-09-04T02:36:53.555997Z","shell.execute_reply.started":"2022-09-04T02:36:53.549441Z","shell.execute_reply":"2022-09-04T02:36:53.554629Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(30, 5))\nfor i, col in enumerate([\"day\", \"donor\",'cell_type',\"technology\"]):\n    _= df_meta[[col]].value_counts().plot.pie(ax=ax[i], autopct='%1.1f%%', ylabel=col)","metadata":{"execution":{"iopub.status.busy":"2022-09-04T02:36:53.558173Z","iopub.execute_input":"2022-09-04T02:36:53.559221Z","iopub.status.idle":"2022-09-04T02:36:54.184507Z","shell.execute_reply.started":"2022-09-04T02:36:53.559168Z","shell.execute_reply":"2022-09-04T02:36:54.183150Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(8, 12))\nfor i, col in enumerate([\"cell_type\", \"day\", \"technology\"]):\n    _= df_meta.groupby([col, 'donor']).size().unstack().plot(\n        ax=ax[i], kind='bar', stacked=True, grid=True\n    )","metadata":{"execution":{"iopub.status.busy":"2022-09-04T02:36:54.186491Z","iopub.execute_input":"2022-09-04T02:36:54.187498Z","iopub.status.idle":"2022-09-04T02:36:54.943016Z","shell.execute_reply.started":"2022-09-04T02:36:54.187456Z","shell.execute_reply":"2022-09-04T02:36:54.941545Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(8, 12))\nfor i, col in enumerate([\"cell_type\", \"donor\", \"technology\"]):\n    _= df_meta.groupby([col, 'day']).size().unstack().plot(\n        ax=ax[i], kind='bar', stacked=True, grid=True\n    )","metadata":{"execution":{"iopub.status.busy":"2022-09-04T02:36:54.944967Z","iopub.execute_input":"2022-09-04T02:36:54.946179Z","iopub.status.idle":"2022-09-04T02:36:55.804885Z","shell.execute_reply.started":"2022-09-04T02:36:54.946126Z","shell.execute_reply":"2022-09-04T02:36:55.803687Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"%%time\ndf_cite_train_x = pd.read_hdf(FP_CITE_TRAIN_INPUTS)\ndisplay(df_cite_train_x.head())\ndf_cite_test_x = pd.read_hdf(FP_CITE_TEST_INPUTS)\nprint('Shapes:', df_cite_train_x.shape, df_cite_test_x.shape)\nprint(\"Missing values:\", df_cite_train_x.isna().sum().sum(), df_cite_test_x.isna().sum().sum())\nprint(\"Genes which never occur in train:\", (df_cite_train_x == 0).all(axis=0).sum())\nprint(\"Genes which never occur in test: \", (df_cite_test_x == 0).all(axis=0).sum())\nprint(f\"Zero entries in train: {(df_cite_train_x == 0).sum().sum() / df_cite_train_x.size:.0%}\")\nprint(f\"Zero entries in test:  {(df_cite_test_x == 0).sum().sum() / df_cite_test_x.size:.0%}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-04T02:36:55.810548Z","iopub.execute_input":"2022-09-04T02:36:55.811199Z","iopub.status.idle":"2022-09-04T02:38:43.310912Z","shell.execute_reply.started":"2022-09-04T02:36:55.811154Z","shell.execute_reply":"2022-09-04T02:38:43.309019Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df_cite_test_x = None\ndf_cite_train_x=None","metadata":{"execution":{"iopub.status.busy":"2022-09-04T02:38:43.317479Z","iopub.execute_input":"2022-09-04T02:38:43.318994Z","iopub.status.idle":"2022-09-04T02:38:43.401154Z","shell.execute_reply.started":"2022-09-04T02:38:43.318925Z","shell.execute_reply":"2022-09-04T02:38:43.399720Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df_cite_train_y = pd.read_hdf(FP_CITE_TRAIN_TARGETS)\ndisplay(df_cite_train_y.head())\nprint('Output shape:', df_cite_train_y.shape)\n\n_, axs = plt.subplots(5, 4, figsize=(16, 16))\nfor col, ax in zip(['CD86', 'CD270', 'CD48', 'CD8', 'CD7', 'CD14', 'CD62L', 'CD54', 'CD42b', 'CD2', 'CD18', 'CD36', 'CD328', 'CD224', 'CD35', 'CD57', 'TCRVd2', 'HLA-E', 'CD82', 'CD101'], axs.ravel()):\n    ax.hist(df_cite_train_y[col], bins=100, density=True)\n    ax.set_title(col)\nplt.tight_layout(h_pad=2)\nplt.suptitle('Selected target histograms', fontsize=20, y=1.04)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-04T02:38:43.403069Z","iopub.execute_input":"2022-09-04T02:38:43.403618Z","iopub.status.idle":"2022-09-04T02:38:50.705733Z","shell.execute_reply.started":"2022-09-04T02:38:43.403575Z","shell.execute_reply":"2022-09-04T02:38:50.704375Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"* We need to notice that the output has 140 columns","metadata":{}},{"cell_type":"code","source":"import os, gc, pickle\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom colorama import Fore, Back, Style\nfrom matplotlib.ticker import MaxNLocator\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler, scale\nfrom sklearn.decomposition import PCA\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet, MultiTaskElasticNetCV,LinearRegression\nfrom sklearn.neural_network import BernoulliRBM\n","metadata":{"execution":{"iopub.status.busy":"2022-09-04T02:42:37.201914Z","iopub.execute_input":"2022-09-04T02:42:37.202430Z","iopub.status.idle":"2022-09-04T02:42:37.211618Z","shell.execute_reply.started":"2022-09-04T02:42:37.202389Z","shell.execute_reply":"2022-09-04T02:42:37.210622Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# 2. To Build up a model for cite first\nIn this competition, we need to build up two seperate models, one for cite, and one for multi","metadata":{}},{"cell_type":"code","source":"def correlation_score(y_true, y_pred):\n    \"\"\"Scores the predictions according to the competition rules. \n    \n    It is assumed that the predictions are not constant.\n    \n    Returns the average of each sample's Pearson correlation coefficient\"\"\"\n    if type(y_true) == pd.DataFrame: y_true = y_true.values\n    if type(y_pred) == pd.DataFrame: y_pred = y_pred.values\n    corrsum = 0\n    for i in range(len(y_true)):\n        corrsum += np.corrcoef(y_true[i], y_pred[i])[1, 0]\n    return corrsum / len(y_true)","metadata":{"execution":{"iopub.status.busy":"2022-09-04T02:38:51.161182Z","iopub.execute_input":"2022-09-04T02:38:51.162033Z","iopub.status.idle":"2022-09-04T02:38:51.169583Z","shell.execute_reply.started":"2022-09-04T02:38:51.161996Z","shell.execute_reply":"2022-09-04T02:38:51.168286Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df_cell = pd.read_csv(FP_CELL_METADATA)\ndf_cell_cite = df_cell[df_cell.technology==\"citeseq\"]\ndf_cell_multi = df_cell[df_cell.technology==\"multiome\"]\ndf_cell_cite.shape, df_cell_multi.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-04T02:38:51.171479Z","iopub.execute_input":"2022-09-04T02:38:51.171838Z","iopub.status.idle":"2022-09-04T02:38:51.635314Z","shell.execute_reply.started":"2022-09-04T02:38:51.171807Z","shell.execute_reply":"2022-09-04T02:38:51.634079Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"%%time\n\nclass PreprocessCiteseq(BaseEstimator, TransformerMixin):\n    columns_to_use = 12000\n    \n    @staticmethod\n    def take_column_subset(X):\n        return X[:,-PreprocessCiteseq.columns_to_use:]\n    \n    def transform(self, X):\n        print(X.shape)\n        X = X[:,~self.all_zero_columns]\n        print(X.shape)\n        X = PreprocessCiteseq.take_column_subset(X) # use only a part of the columns\n        print(X.shape)\n        gc.collect()\n\n        X = self.pca.transform(X)\n        print(X.shape)\n        return X\n\n    def fit_transform(self, X):\n        gc.collect()\n        print(X.shape)\n        self.all_zero_columns = (X == 0).all(axis=0)\n        X = X[:,~self.all_zero_columns]\n        print(X.shape)\n        X = PreprocessCiteseq.take_column_subset(X) # use only a part of the columns\n        print(X.shape)\n        gc.collect()\n\n        self.pca = PCA(n_components=240, copy=False, random_state=1)\n        X = self.pca.fit_transform(X)\n        plt.plot(self.pca.explained_variance_ratio_.cumsum())\n        plt.title(\"Cumulative explained variance ratio\")\n        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n        plt.xlabel('PCA component')\n        plt.ylabel('Cumulative explained variance ratio')\n        plt.show()\n        print(X.shape)\n        return X\n\npreprocessor = PreprocessCiteseq()\n\ncite_train_x = None\ncite_train_x = preprocessor.fit_transform(pd.read_hdf(FP_CITE_TRAIN_INPUTS).values)\n\ncite_train_y = pd.read_hdf(FP_CITE_TRAIN_TARGETS).values\nprint(cite_train_y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-09-04T02:38:51.636799Z","iopub.execute_input":"2022-09-04T02:38:51.638029Z","iopub.status.idle":"2022-09-04T02:41:06.487570Z","shell.execute_reply.started":"2022-09-04T02:38:51.637983Z","shell.execute_reply":"2022-09-04T02:41:06.486104Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"%%time\n# Cross-validation(Ridge)\nmodel = None\n\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\nscore_list = []\nfor fold, (idx_tr, idx_va) in enumerate(kf.split(cite_train_x)):\n    model = None\n    gc.collect()\n    X_tr = cite_train_x[idx_tr] \n    y_tr = cite_train_y[idx_tr]\n\n    model = Ridge(copy_X=False) # overwrites the copied data\n    model.fit(X_tr, y_tr)\n    del X_tr, y_tr\n    gc.collect()\n\n    # We validate the model\n    X_va = cite_train_x[idx_va]\n    y_va = cite_train_y[idx_va]\n    y_va_pred = model.predict(X_va)\n    mse = mean_squared_error(y_va, y_va_pred)\n    corrscore = correlation_score(y_va, y_va_pred)\n    del X_va, y_va\n\n    print(f\"Fold {fold}: mse = {mse:.5f}, corr =  {corrscore:.3f}\")\n    score_list.append((mse, corrscore))\n\n# Show overall score\nresult_df = pd.DataFrame(score_list, columns=['mse', 'corrscore'])\nprint(f\"{Fore.GREEN}{Style.BRIGHT}Average  mse = {result_df.mse.mean():.5f}; corr = {result_df.corrscore.mean():.3f}{Style.RESET_ALL}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-04T02:44:57.571241Z","iopub.execute_input":"2022-09-04T02:44:57.571663Z","iopub.status.idle":"2022-09-04T02:45:08.713845Z","shell.execute_reply.started":"2022-09-04T02:44:57.571631Z","shell.execute_reply":"2022-09-04T02:45:08.712338Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model = None\n\n# Cross-validation (Lasso)\n\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\nscore_list = []\nfor fold, (idx_tr, idx_va) in enumerate(kf.split(cite_train_x)):\n    model = None\n    gc.collect()\n    X_tr = cite_train_x[idx_tr] \n    y_tr = cite_train_y[idx_tr]\n\n    model = Lasso(alpha=1.7,copy_X=False) \n    model.fit(X_tr, y_tr)\n    del X_tr, y_tr\n    gc.collect()\n\n    # We validate the model\n    X_va = cite_train_x[idx_va]\n    y_va = cite_train_y[idx_va]\n    y_va_pred = model.predict(X_va)\n    mse = mean_squared_error(y_va, y_va_pred)\n    corrscore = correlation_score(y_va, y_va_pred)\n    del X_va, y_va\n\n    print(f\"Fold {fold}: mse = {mse:.5f}, corr =  {corrscore:.3f}\")\n    score_list.append((mse, corrscore))\n\n# Show overall score\nresult_df = pd.DataFrame(score_list, columns=['mse', 'corrscore'])\nprint(f\"{Fore.GREEN}{Style.BRIGHT}Average  mse = {result_df.mse.mean():.5f}; corr = {result_df.corrscore.mean():.3f}{Style.RESET_ALL}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-04T02:46:47.625553Z","iopub.execute_input":"2022-09-04T02:46:47.626251Z","iopub.status.idle":"2022-09-04T02:47:45.357375Z","shell.execute_reply.started":"2022-09-04T02:46:47.626198Z","shell.execute_reply":"2022-09-04T02:47:45.354946Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"%%time\n# Cross-validation (ElasticNet)\nmodel=None\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\nscore_list = []\nfor fold, (idx_tr, idx_va) in enumerate(kf.split(cite_train_x)):\n    model = None\n    gc.collect()\n    X_tr = cite_train_x[idx_tr]\n    y_tr = cite_train_y[idx_tr]\n\n    model = ElasticNet(copy_X=False) # overwrites the copied data\n    model.fit(X_tr, y_tr)\n    del X_tr, y_tr\n    gc.collect()\n\n    # We validate the model\n    X_va = cite_train_x[idx_va]\n    y_va = cite_train_y[idx_va]\n    y_va_pred = model.predict(X_va)\n    mse = mean_squared_error(y_va, y_va_pred)\n    corrscore = correlation_score(y_va, y_va_pred)\n    del X_va, y_va\n\n    print(f\"Fold {fold}: mse = {mse:.5f}, corr =  {corrscore:.3f}\")\n    score_list.append((mse, corrscore))\n\n# Show overall score\nresult_df = pd.DataFrame(score_list, columns=['mse', 'corrscore'])\nprint(f\"{Fore.GREEN}{Style.BRIGHT}Average  mse = {result_df.mse.mean():.5f}; corr = {result_df.corrscore.mean():.3f}{Style.RESET_ALL}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-04T02:47:56.412757Z","iopub.execute_input":"2022-09-04T02:47:56.413170Z","iopub.status.idle":"2022-09-04T02:48:53.647729Z","shell.execute_reply.started":"2022-09-04T02:47:56.413136Z","shell.execute_reply":"2022-09-04T02:48:53.646393Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"class PreprocessMultiome(BaseEstimator, TransformerMixin):\n    columns_to_use = slice(10000, 14000)\n    \n    @staticmethod\n    def take_column_subset(X):\n        return X[:,PreprocessMultiome.columns_to_use]\n    \n    def transform(self, X):\n        print(X.shape)\n        X = X[:,~self.all_zero_columns]\n        print(X.shape)\n        X = PreprocessMultiome.take_column_subset(X) # use only a part of the columns\n        print(X.shape)\n        gc.collect()\n\n        X = self.pca.transform(X)\n        print(X.shape)\n        return X\n\n    def fit_transform(self, X):\n        print(X.shape)\n        self.all_zero_columns = (X == 0).all(axis=0)\n        X = X[:,~self.all_zero_columns]\n        print(X.shape)\n        X = PreprocessMultiome.take_column_subset(X) # use only a part of the columns\n        print(X.shape)\n        gc.collect()\n\n        self.pca = PCA(n_components=4, copy=False, random_state=1)\n        X = self.pca.fit_transform(X)\n        plt.plot(self.pca.explained_variance_ratio_.cumsum())\n        plt.title(\"Cumulative explained variance ratio\")\n        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n        plt.xlabel('PCA component')\n        plt.ylabel('Cumulative explained variance ratio')\n        plt.show()\n        print(X.shape)\n        return X\n\npreprocessor = PreprocessMultiome()\n\nmulti_train_x = None\nstart, stop = 0, 6000\nmulti_train_x = preprocessor.fit_transform(pd.read_hdf(FP_MULTIOME_TRAIN_INPUTS, start=start, stop=stop).values)\n\nmulti_train_y = pd.read_hdf(FP_MULTIOME_TRAIN_TARGETS, start=start, stop=stop)\ny_columns = multi_train_y.columns\nmulti_train_y = multi_train_y.values\nprint(multi_train_y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-09-04T02:55:44.516565Z","iopub.execute_input":"2022-09-04T02:55:44.519111Z","iopub.status.idle":"2022-09-04T02:56:18.842671Z","shell.execute_reply.started":"2022-09-04T02:55:44.519031Z","shell.execute_reply":"2022-09-04T02:56:18.841372Z"},"trusted":true},"execution_count":27,"outputs":[]}]}